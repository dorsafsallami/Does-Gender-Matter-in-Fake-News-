{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol4qB3Vg7xY1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install constant\n",
        "!pip install bert-pytorch\n",
        "!pip install pytorch-pretrained-bert pytorch-nlp\n",
        "!pip install -U -q PyDrive"
      ],
      "metadata": {
        "id": "5SJK350Qo6Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.models import Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, sys\n",
        "import constant"
      ],
      "metadata": {
        "id": "CjeU8rpAowi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import warnings\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam, BertModel\n",
        "from pytorch_pretrained_bert import BertConfig\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "id": "Tb5gZZ6Mo3t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "YMIzYppjpHUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "LJ6pVLhzpKl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Ne3d3nUXqyQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FakeNewsNet"
      ],
      "metadata": {
        "id": "k71BgJyywGDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Gender Bias/FakeNewsNet2.csv')"
      ],
      "metadata": {
        "id": "5Ln3x1popMsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {'fake': 1, 'real': 0}\n",
        "df['label'] = df['label'].map(label_mapping)"
      ],
      "metadata": {
        "id": "myX75tKcri9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['female'] = df['gender'].apply(lambda x: 1 if x == 'female' else 0)\n",
        "df['male'] = df['gender'].apply(lambda x: 1 if x == 'male' else 0)"
      ],
      "metadata": {
        "id": "zB4W95HTqJSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_total , df_val_total = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "DtvKiaqmqPyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_total"
      ],
      "metadata": {
        "id": "7cZajfOXqzq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train_total\n",
        "df_val = df_val_total"
      ],
      "metadata": {
        "id": "FpDGvhnsrEJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments_train = df_train.title\n",
        "comments_val = df_val.title"
      ],
      "metadata": {
        "id": "TjFQRIEPq1zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_female_gender(x):\n",
        "  if np.isnan(x.female) or x.female < 0.5:\n",
        "      return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "Bt96iDJ7rAP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unprotected_class(list_of_protected):\n",
        "  new = [1 if i == 0 else 0 for i in list_of_protected]\n",
        "  return new"
      ],
      "metadata": {
        "id": "IWBQFBJtrK8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(labels, preds):\n",
        "  pred_flat = preds.flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "\n",
        "  acc = accuracy_score(labels_flat, pred_flat)\n",
        "  pre = precision_score(labels_flat, pred_flat)\n",
        "  rec = recall_score(labels_flat, pred_flat)\n",
        "  f1 = f1_score(labels_flat, pred_flat, average=\"weighted\")\n",
        "\n",
        "  return acc, pre, rec, f1"
      ],
      "metadata": {
        "id": "ivTv_BnfrNYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fakness_labels_train = list(df_train.label.apply(lambda x: 1 if x >= 0.5 else 0))\n",
        "identity_labels_train = list(df_train.apply(extract_female_gender, axis = 1))\n",
        "fakness_labels_val = list(df_val.label.apply(lambda x: 1 if x >= 0.5 else 0))\n",
        "identity_labels_val = list(df_val.apply(extract_female_gender, axis = 1))\n",
        "unprotected_labels_train = get_unprotected_class(identity_labels_train)\n",
        "unprotected_labels_val = get_unprotected_class(identity_labels_val)"
      ],
      "metadata": {
        "id": "blkzrJ3wrQEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(comments_train), len(fakness_labels_train))\n",
        "print(comments_train[:10])\n",
        "print(fakness_labels_train[:10])\n",
        "print(identity_labels_train[:10])"
      ],
      "metadata": {
        "id": "vaZoyJciroHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 128\n",
        "SEED = 519\n",
        "BATCH_SIZE = 32\n",
        "BERT_MODEL_PATH = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)"
      ],
      "metadata": {
        "id": "GKtQnyRSrs2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_lines(example, max_seq_length,tokenizer):\n",
        "\n",
        "    all_tokens = []\n",
        "    longer = 0\n",
        "    for text in tqdm(example):\n",
        "        tokens_a = tokenizer.tokenize(text)\n",
        "        if len(tokens_a)>max_seq_length:\n",
        "            tokens_a = tokens_a[:max_seq_length]\n",
        "            longer += 1\n",
        "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
        "        all_tokens.append(one_token)\n",
        "    print(\"Tokens longer than max_length: \", longer)\n",
        "    return np.array(all_tokens)\n"
      ],
      "metadata": {
        "id": "HZCau4Yhrwf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_train = convert_lines(comments_train.fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
        "fakness_labels_train = torch.tensor(fakness_labels_train)\n",
        "female_labels_train = torch.tensor(identity_labels_train)\n",
        "\n",
        "input_val = convert_lines(comments_val.fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
        "fakness_labels_val = torch.tensor(fakness_labels_val)\n",
        "female_labels_val = torch.tensor(identity_labels_val)\n"
      ],
      "metadata": {
        "id": "iMDtTSfpryxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.sum(fakness_labels_train).data)\n",
        "print(torch.sum(female_labels_train).data)\n",
        "\n",
        "print(torch.sum(fakness_labels_val).data)\n",
        "print(torch.sum(female_labels_val).data)\n"
      ],
      "metadata": {
        "id": "DcMjHpNvr9Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.utils.data.TensorDataset(torch.tensor(input_train, dtype=torch.long), fakness_labels_train, female_labels_train)\n",
        "train_loader = torch.utils.data.DataLoader(X_train, batch_size=32, shuffle=True)\n",
        "\n",
        "X_val = torch.utils.data.TensorDataset(torch.tensor(input_val, dtype=torch.long), fakness_labels_val, female_labels_val)\n",
        "val_loader = torch.utils.data.DataLoader(X_val, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "GTonlTRgsCYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fairness_metrics(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "  def get_fake_rates(y_pred, protected_labels, non_protected_labels, thres):\n",
        "    protected_ops = y_pred[protected_labels == 1]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[non_protected_labels == 1]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return np.round(protected_prob, 2), np.round(non_protected_prob, 2)\n",
        "\n",
        "  def get_true_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "    protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels == 1)]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 1)]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return np.round(protected_prob, 2), np.round(non_protected_prob, 2)\n",
        "\n",
        "\n",
        "  def get_false_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "    protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels ==0)]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 0)]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return np.round(protected_prob, 2), np.round(non_protected_prob, 2)\n",
        "\n",
        "  def demographic_parity(y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "    protected_ops = y_pred[protected_labels == 1]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[non_protected_labels == 1]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return abs(protected_prob - non_protected_prob)\n",
        "\n",
        "  def true_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "    protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels == 1)]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 1)]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return abs(protected_prob - non_protected_prob)\n",
        "\n",
        "\n",
        "  def false_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "    protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels ==0)]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 0)]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return abs(protected_prob - non_protected_prob)\n",
        "\n",
        "\n",
        "  def equalized_odds(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "    return true_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres) + false_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "\n",
        "  female_tox_rate, nf_tox_rate = get_fake_rates(y_pred, protected_labels, non_protected_labels, thres)\n",
        "  female_tp_rate, nf_tp_rate = get_true_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "  female_fp_rate, nf_fp_rate = get_false_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "  demo_parity = demographic_parity(y_pred, protected_labels, non_protected_labels, thres)\n",
        "  tp_parity = true_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "  fp_parity = false_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "  equ_odds = equalized_odds(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "\n",
        "  return female_tox_rate, nf_tox_rate, female_tp_rate, nf_tp_rate, female_fp_rate, nf_fp_rate, demo_parity, tp_parity, fp_parity, equ_odds\n",
        "\n"
      ],
      "metadata": {
        "id": "mK96hsrqsJOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072,\n",
        "        hidden_dropout_prob=0.1)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, fakness_labels = 2):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.c1 = nn.Linear(config.hidden_size, 324)\n",
        "        self.c3 = nn.Linear(324, fakness_labels)\n",
        "\n",
        "        nn.init.xavier_normal_(self.c1.weight)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "\n",
        "\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "\n",
        "        classifier_prev_output = F.relu(self.c1(pooled_output))\n",
        "        classifier_output = self.c3(classifier_prev_output)\n",
        "\n",
        "        return classifier_output, classifier_prev_output\n",
        "\n",
        "class Adversary(nn.Module):\n",
        "    def __init__(self, identity_labels = 2):\n",
        "        super(Adversary, self).__init__()\n",
        "\n",
        "        self.a1 = nn.Linear(324,120)\n",
        "        self.a2 = nn.Linear(120, identity_labels)\n",
        "\n",
        "        nn.init.xavier_normal_(self.a1.weight)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        adversary = F.relu(self.a1(input_ids))\n",
        "        adversary_output = self.a2(adversary)\n",
        "\n",
        "        return adversary_output\n"
      ],
      "metadata": {
        "id": "PRGQtomTsNrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conduct_validation(net, data_loader, adv = False):\n",
        "\n",
        "    eval_loss, eval_accuracy, eval_precision, eval_recall, eval_f1 = 0, 0, 0, 0, 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    predictions_net = np.empty((0,))\n",
        "    truths = np.empty((0,))\n",
        "    identities = np.empty((0,))\n",
        "    correct_net = 0\n",
        "    total = 0\n",
        "\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "      for index, data in enumerate(data_loader):\n",
        "\n",
        "        text, fake_truth, female_truth = data\n",
        "\n",
        "        text = text.to(device)\n",
        "        fake_truth = fake_truth.to(device)\n",
        "        female_truth = female_truth.to(device)\n",
        "\n",
        "        if adv:\n",
        "          net_outputs, net_prev_outputs = net(text)\n",
        "        else:\n",
        "          net_outputs = net(text)\n",
        "        _, net_predicted = torch.max(net_outputs.data, 1)\n",
        "\n",
        "        batch_size = fake_truth.size(0)\n",
        "        total += batch_size\n",
        "        correct_net_batch = (net_predicted == fake_truth).sum().item()\n",
        "        correct_net += correct_net_batch\n",
        "\n",
        "\n",
        "        predictions_net = np.concatenate((predictions_net, net_predicted.cpu().numpy()))\n",
        "        truths = np.concatenate((truths, fake_truth.cpu().numpy()))\n",
        "        identities = np.concatenate((identities, female_truth.cpu().numpy()))\n",
        "\n",
        "        pred = net_predicted.detach().cpu().numpy()\n",
        "        label_ids = fake_truth.to('cpu').numpy()\n",
        "\n",
        "        tmp_eval_accuracy, tmp_eval_precision, temp_eval_recall, tmp_eval_f1 = get_metrics(label_ids, pred)\n",
        "\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        eval_precision += tmp_eval_precision\n",
        "        eval_recall += temp_eval_recall\n",
        "        eval_f1 += tmp_eval_f1\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    f1_score = eval_f1/nb_eval_steps\n",
        "    prec_score = eval_precision/nb_eval_steps\n",
        "    recall_score = eval_recall/nb_eval_steps\n",
        "    acc_score = eval_accuracy/nb_eval_steps\n",
        "\n",
        "    print(\"F1 Score: \", f1_score)\n",
        "    print(\"Precision Score: \", prec_score)\n",
        "    print(\"Recall Score: \", recall_score)\n",
        "    print(\"Acc Score: \", acc_score, \"\\n\\n\")\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    return (predictions_net, truths, identities, acc_score)"
      ],
      "metadata": {
        "id": "C4y_oh0rsQO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_classifier(clf, optimizer_clf, train_loader, loss_criterion, epochs):\n",
        "\n",
        "  pretrain_classifier_loss = 0\n",
        "  steps = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    print(\"Epoch: \", epoch + 1)\n",
        "    epoch_loss = 0\n",
        "    epoch_batches = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "\n",
        "        inputs, fake_true, female_true = data\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        fake_true = fake_true.to(device)\n",
        "        female_true = female_true.to(device)\n",
        "\n",
        "        optimizer_clf.zero_grad()\n",
        "\n",
        "        classifier_output, _ = clf(inputs)\n",
        "        classifier_loss = loss_criterion(classifier_output, fake_true)\n",
        "        classifier_loss.backward()\n",
        "        optimizer_clf.step()\n",
        "        pretrain_classifier_loss += classifier_loss.item()\n",
        "        epoch_loss += classifier_loss.item()\n",
        "        epoch_batches += 1\n",
        "        steps += 1\n",
        "\n",
        "    print(\"Average Pretrain Classifier epoch loss: \", epoch_loss/epoch_batches)\n",
        "  print(\"Average Pretrain Classifier batch loss: \", pretrain_classifier_loss/steps)\n",
        "\n",
        "  return clf"
      ],
      "metadata": {
        "id": "QLnXt_XTsxhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, epochs):\n",
        "\n",
        "  pretrain_adversary_loss = 0\n",
        "  steps = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    print(\"Epoch: \", epoch + 1)\n",
        "    epoch_loss = 0\n",
        "    epoch_batches = 0\n",
        "    for i, data in enumerate(train_loader):\n",
        "\n",
        "        inputs, fake_true, female_true = data\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        fake_true = fake_true.to(device)\n",
        "        female_true = female_true.to(device)\n",
        "\n",
        "        optimizer_adv.zero_grad()\n",
        "\n",
        "        _, classifier_prev_output = clf(inputs)\n",
        "        adversary_output = adv(classifier_prev_output)\n",
        "        adversary_loss = loss_criterion(adversary_output, female_true)\n",
        "        adversary_loss.backward()\n",
        "        optimizer_adv.step()\n",
        "        pretrain_adversary_loss += adversary_loss.item()\n",
        "        epoch_loss += adversary_loss.item()\n",
        "        epoch_batches += 1\n",
        "        steps += 1\n",
        "\n",
        "    print(\"Average Pretrain Adversary epoch loss: \", epoch_loss/epoch_batches)\n",
        "  print(\"Average Pretrain Adversary batch loss: \", pretrain_adversary_loss/steps)\n",
        "\n",
        "  return adv"
      ],
      "metadata": {
        "id": "gYQTV6nntDNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, epochs=1):\n",
        "\n",
        "  adv_loss = 0\n",
        "  steps = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for i, data in enumerate(train_loader):\n",
        "\n",
        "        inputs, fake_true, female_true = data\n",
        "        inputs = inputs.to(device)\n",
        "        fake_true = fake_true.to(device)\n",
        "        female_true = female_true.to(device)\n",
        "\n",
        "        optimizer_adv.zero_grad()\n",
        "\n",
        "        classifier_output, classifier_prev_output = clf(inputs)\n",
        "        adversary_output = adv(classifier_prev_output)\n",
        "        adversary_loss = loss_criterion(adversary_output, female_true)\n",
        "        adversary_loss.backward()\n",
        "        optimizer_adv.step()\n",
        "        adv_loss += adversary_loss.item()\n",
        "        steps += 1\n",
        "\n",
        "  print(\"Average Adversary batch loss: \", adv_loss/steps)\n",
        "\n",
        "  return adv"
      ],
      "metadata": {
        "id": "90dl5YsUtMD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(clf, optimizer_clf, adv, train_loader, loss_criterion, lbda):\n",
        "\n",
        "  for i, data in enumerate(train_loader):\n",
        "\n",
        "      inputs, fake_true, female_true = data\n",
        "      inputs = inputs.to(device)\n",
        "\n",
        "      fake_true = fake_true.to(device)\n",
        "      female_true = female_true.to(device)\n",
        "\n",
        "\n",
        "      optimizer_clf.zero_grad()\n",
        "\n",
        "      classifier_output, classifier_prev_output = clf(inputs)\n",
        "      adversary_output = adv(classifier_prev_output)\n",
        "      adversary_loss = loss_criterion(adversary_output, female_true)\n",
        "      classifier_loss = loss_criterion(classifier_output, fake_true)\n",
        "      total_classifier_loss = classifier_loss - lbda * adversary_loss\n",
        "      total_classifier_loss.backward()\n",
        "\n",
        "      optimizer_clf.step()\n",
        "\n",
        "      print(\"Adversary Mini-Batch loss: \", adversary_loss.item())\n",
        "      print(\"Classifier Mini-Batch loss: \", classifier_loss.item())\n",
        "      print(\"Total Mini-Batch loss: \", total_classifier_loss.item())\n",
        "\n",
        "      break\n",
        "\n",
        "  return clf"
      ],
      "metadata": {
        "id": "V-DMbGWdtT3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_params = [0.1, 0.2, 0.3, 0.5 1,2,3,5,7,10]\n",
        "\n",
        "lbda_train_accs = []\n",
        "lbda_valid_accs = []\n",
        "protected_fake_rates = []\n",
        "unprotected_fake_rates = []\n",
        "protected_tp_rates = []\n",
        "unprotected_tp_rates = []\n",
        "protected_fp_rates = []\n",
        "unprotected_fp_rates = []\n",
        "demo_parity_scores = []\n",
        "tp_parity_scores = []\n",
        "fp_parity_scores = []\n",
        "equ_odds_scores = []\n",
        "\n",
        "for lbda in lambda_params:\n",
        "\n",
        "\n",
        "  clf = Classifier(fakness_labels = 2)\n",
        "  adv = Adversary(identity_labels = 2)\n",
        "\n",
        "  loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "  optimizer_adv = optim.Adam(adv.parameters(), lr=0.001)\n",
        "\n",
        "  lrlast = .001\n",
        "  lrmain = .00001\n",
        "  optimizer_clf = optim.Adam(\n",
        "      [\n",
        "          {\"params\":clf.bert.parameters(),\"lr\": lrmain},\n",
        "          {\"params\":clf.c1.parameters(), \"lr\": lrlast},\n",
        "\n",
        "      {\"params\":clf.c3.parameters(), \"lr\": lrlast}\n",
        "    ])\n",
        "\n",
        "  clf.to(device)\n",
        "  adv.to(device)\n",
        "\n",
        "\n",
        "\n",
        "  for param in adv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  clf = pretrain_classifier(clf, optimizer_clf, train_loader, loss_criterion, 3)\n",
        "\n",
        "  for param in adv.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "  for param in clf.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  adv = pretrain_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, 3)\n",
        "\n",
        "  for param in clf.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "  print('Lambda: ' + str(lbda))\n",
        "\n",
        "  train_accs = []\n",
        "  valid_accs = []\n",
        "  iterations = 20\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "      print(\"Iteration: \", iteration)\n",
        "\n",
        "\n",
        "\n",
        "      for param in clf.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "      adv = train_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, epochs=1)\n",
        "\n",
        "      for param in clf.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "\n",
        "      for param in adv.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "      clf = train_classifier(clf, optimizer_clf, adv, train_loader, loss_criterion, lbda)\n",
        "\n",
        "      for param in adv.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "      if (iteration + 1) % 2 == 0:\n",
        "\n",
        "        print('Training metrics:')\n",
        "        y_pred, actual_labels, protected_labels, acc_score = conduct_validation(clf, train_loader, adv = True)\n",
        "        train_accs.append(acc_score)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Fairness Metrics on Train:\")\n",
        "        non_protected_labels = np.asarray(get_unprotected_class(protected_labels))\n",
        "        thres = 0.5\n",
        "        female_tox_rate, nf_tox_rate, female_tp_rate, nf_tp_rate, female_fp_rate, nf_fp_rate, demo_parity, tp_parity, fp_parity, equ_odds =\\\n",
        "        get_fairness_metrics(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "\n",
        "        print(\"Fake Prediction Rates: \", \"Female -\", female_tox_rate, \"Non-Female - \", nf_tox_rate)\n",
        "        print(\"True Positive Prediction Rates: \", \"Female -\", female_tp_rate, \"Non-Female - \", nf_tp_rate)\n",
        "        print(\"False Positive Prediction Rates: \", \"Female -\", female_fp_rate, \"Non-Female - \", nf_fp_rate)\n",
        "        print(\"Demographic Parity: \", demo_parity)\n",
        "        print(\"True Positive Parity: \", tp_parity)\n",
        "        print(\"False Positive Parity: \", fp_parity)\n",
        "        print(\"Equalized Odds: \", equ_odds)\n",
        "        print(\"\\n\")\n",
        "        print('Validation metrics:')\n",
        "        y_pred, actual_labels, protected_labels, acc_score = conduct_validation(clf, val_loader, adv = True)\n",
        "        valid_accs.append(acc_score)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Fairness Metrics on Validation:\")\n",
        "        non_protected_labels = np.asarray(get_unprotected_class(protected_labels))\n",
        "        thres = 0.5\n",
        "        female_tox_rate, nf_tox_rate, female_tp_rate, nf_tp_rate, female_fp_rate, nf_fp_rate, demo_parity, tp_parity, fp_parity, equ_odds =\\\n",
        "        get_fairness_metrics(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "\n",
        "        print(\"Fake Prediction Rates: \", \"Female -\", female_tox_rate, \"Non-Female - \", nf_tox_rate)\n",
        "        print(\"True Positive Prediction Rates: \", \"Female -\", female_tp_rate, \"Non-Female - \", nf_tp_rate)\n",
        "        print(\"False Positive Prediction Rates: \", \"Female -\", female_fp_rate, \"Non-Female - \", nf_fp_rate)\n",
        "        print(\"Demographic Parity: \", demo_parity)\n",
        "        print(\"True Positive Parity: \", tp_parity)\n",
        "        print(\"False Positive Parity: \", fp_parity)\n",
        "        print(\"Equalized Odds: \", equ_odds)\n",
        "        print(\"\\n\\n\\n__________________\")\n",
        "\n",
        "        if iteration == iterations -1:\n",
        "          protected_fake_rates.append(female_tox_rate)\n",
        "          unprotected_fake_rates.append(nf_tox_rate)\n",
        "          protected_tp_rates.append(female_tp_rate)\n",
        "          unprotected_tp_rates.append(nf_tp_rate)\n",
        "          protected_fp_rates.append(female_fp_rate)\n",
        "          unprotected_fp_rates.append(nf_fp_rate)\n",
        "          demo_parity_scores.append(demo_parity)\n",
        "          tp_parity_scores.append(tp_parity)\n",
        "          fp_parity_scores.append(fp_parity)\n",
        "          equ_odds_scores.append(equ_odds)\n",
        "\n",
        "  lbda_train_accs.append(train_accs)\n",
        "  lbda_valid_accs.append(valid_accs)\n",
        "\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "HNNtz5zPtb6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(clf.state_dict(), \"/content/drive/MyDrive/Gender Bias/saved_models/SC_Classifier_Final_FakeNewsNet\")\n",
        "torch.save(adv.state_dict(), \"/content/drive/MyDrive/Gender Bias/saved_models/SC_Adversary_Final_FakeNewsNet\")"
      ],
      "metadata": {
        "id": "XnpNrp8DuI1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COVID19"
      ],
      "metadata": {
        "id": "_pAch-JhwMNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv ('/content/drive/MyDrive/Gender Bias/covid.csv')"
      ],
      "metadata": {
        "id": "WtXsikWswMN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {'fake': 1, 'real': 0}\n",
        "df['label'] = df['label'].map(label_mapping)"
      ],
      "metadata": {
        "id": "qXhutMwrwMN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['female'] = df['gender'].apply(lambda x: 1 if x == 'female' else 0)\n",
        "df['male'] = df['gender'].apply(lambda x: 1 if x == 'male' else 0)"
      ],
      "metadata": {
        "id": "rMto8qh_wMN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_total , df_val_total = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "FkY5Gdu5wMN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_total"
      ],
      "metadata": {
        "id": "yCjLHbumwMN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train_total\n",
        "df_val = df_val_total"
      ],
      "metadata": {
        "id": "Kap7xE1-wMN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments_train = df_train.tweet\n",
        "comments_val = df_val.tweet"
      ],
      "metadata": {
        "id": "BAWfr16RwMN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_female_gender(x):\n",
        "  if np.isnan(x.female) or x.female < 0.5:\n",
        "      return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "fd-K8eulwMN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unprotected_class(list_of_protected):\n",
        "  new = [1 if i == 0 else 0 for i in list_of_protected]\n",
        "  return new"
      ],
      "metadata": {
        "id": "rOfl-vDRwMN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(labels, preds):\n",
        "  pred_flat = preds.flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "\n",
        "  acc = accuracy_score(labels_flat, pred_flat)\n",
        "  pre = precision_score(labels_flat, pred_flat)\n",
        "  rec = recall_score(labels_flat, pred_flat)\n",
        "  f1 = f1_score(labels_flat, pred_flat, average=\"weighted\")\n",
        "\n",
        "  return acc, pre, rec, f1"
      ],
      "metadata": {
        "id": "bj3yFV3cwMN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fakness_labels_train = list(df_train.label.apply(lambda x: 1 if x >= 0.5 else 0))\n",
        "identity_labels_train = list(df_train.apply(extract_female_gender, axis = 1))\n",
        "fakness_labels_val = list(df_val.label.apply(lambda x: 1 if x >= 0.5 else 0))\n",
        "identity_labels_val = list(df_val.apply(extract_female_gender, axis = 1))\n",
        "unprotected_labels_train = get_unprotected_class(identity_labels_train)\n",
        "unprotected_labels_val = get_unprotected_class(identity_labels_val)"
      ],
      "metadata": {
        "id": "DqgBRHj0wMN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(comments_train), len(fakness_labels_train))\n",
        "print(comments_train[:10])\n",
        "print(fakness_labels_train[:10])\n",
        "print(identity_labels_train[:10])"
      ],
      "metadata": {
        "id": "fO1JZmkJwMN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 128\n",
        "SEED = 519\n",
        "BATCH_SIZE = 32\n",
        "BERT_MODEL_PATH = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)"
      ],
      "metadata": {
        "id": "jDJey0eMwMN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_lines(example, max_seq_length,tokenizer):\n",
        "\n",
        "    all_tokens = []\n",
        "    longer = 0\n",
        "    for text in tqdm(example):\n",
        "        tokens_a = tokenizer.tokenize(text)\n",
        "        if len(tokens_a)>max_seq_length:\n",
        "            tokens_a = tokens_a[:max_seq_length]\n",
        "            longer += 1\n",
        "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
        "        all_tokens.append(one_token)\n",
        "    print(\"Tokens longer than max_length: \", longer)\n",
        "    return np.array(all_tokens)\n"
      ],
      "metadata": {
        "id": "K3wCLxYdwMN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_train = convert_lines(comments_train.fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
        "fakness_labels_train = torch.tensor(fakness_labels_train)\n",
        "female_labels_train = torch.tensor(identity_labels_train)\n",
        "\n",
        "input_val = convert_lines(comments_val.fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
        "fakness_labels_val = torch.tensor(fakness_labels_val)\n",
        "female_labels_val = torch.tensor(identity_labels_val)\n"
      ],
      "metadata": {
        "id": "wC0I5O1qwMN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.sum(fakness_labels_train).data)\n",
        "print(torch.sum(female_labels_train).data)\n",
        "\n",
        "print(torch.sum(fakness_labels_val).data)\n",
        "print(torch.sum(female_labels_val).data)\n"
      ],
      "metadata": {
        "id": "gDvgCDDkwMN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.utils.data.TensorDataset(torch.tensor(input_train, dtype=torch.long), fakness_labels_train, female_labels_train)\n",
        "train_loader = torch.utils.data.DataLoader(X_train, batch_size=32, shuffle=True)\n",
        "\n",
        "X_val = torch.utils.data.TensorDataset(torch.tensor(input_val, dtype=torch.long), fakness_labels_val, female_labels_val)\n",
        "val_loader = torch.utils.data.DataLoader(X_val, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Z1WXOUxnwMN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fairness_metrics(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "  def get_fake_rates(y_pred, protected_labels, non_protected_labels, thres):\n",
        "    protected_ops = y_pred[protected_labels == 1]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[non_protected_labels == 1]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return np.round(protected_prob, 2), np.round(non_protected_prob, 2)\n",
        "\n",
        "  def get_true_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "    protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels == 1)]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 1)]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return np.round(protected_prob, 2), np.round(non_protected_prob, 2)\n",
        "\n",
        "\n",
        "  def get_false_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "    protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels ==0)]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 0)]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return np.round(protected_prob, 2), np.round(non_protected_prob, 2)\n",
        "\n",
        "  def demographic_parity(y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "    protected_ops = y_pred[protected_labels == 1]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[non_protected_labels == 1]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return abs(protected_prob - non_protected_prob)\n",
        "  def true_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "    protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels == 1)]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 1)]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return abs(protected_prob - non_protected_prob)\n",
        "\n",
        "\n",
        "  def false_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "    protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels ==0)]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 0)]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return abs(protected_prob - non_protected_prob)\n",
        "\n",
        "  def equalized_odds(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "    return true_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres) + false_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "\n",
        "  female_tox_rate, nf_tox_rate = get_fake_rates(y_pred, protected_labels, non_protected_labels, thres)\n",
        "  female_tp_rate, nf_tp_rate = get_true_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "  female_fp_rate, nf_fp_rate = get_false_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "  demo_parity = demographic_parity(y_pred, protected_labels, non_protected_labels, thres)\n",
        "  tp_parity = true_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "  fp_parity = false_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "  equ_odds = equalized_odds(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "\n",
        "  return female_tox_rate, nf_tox_rate, female_tp_rate, nf_tp_rate, female_fp_rate, nf_fp_rate, demo_parity, tp_parity, fp_parity, equ_odds\n",
        "\n"
      ],
      "metadata": {
        "id": "BMUU34CgwMN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072,\n",
        "        hidden_dropout_prob=0.1)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, fakness_labels = 2):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.c1 = nn.Linear(config.hidden_size, 324)\n",
        "\n",
        "        self.c3 = nn.Linear(324, fakness_labels)\n",
        "\n",
        "        nn.init.xavier_normal_(self.c1.weight)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "\n",
        "\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "\n",
        "        classifier_prev_output = F.relu(self.c1(pooled_output))\n",
        "\n",
        "        classifier_output = self.c3(classifier_prev_output)\n",
        "\n",
        "        return classifier_output, classifier_prev_output\n",
        "\n",
        "class Adversary(nn.Module):\n",
        "    def __init__(self, identity_labels = 2):\n",
        "        super(Adversary, self).__init__()\n",
        "\n",
        "        self.a1 = nn.Linear(324,120)\n",
        "        self.a2 = nn.Linear(120, identity_labels)\n",
        "\n",
        "        nn.init.xavier_normal_(self.a1.weight)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        adversary = F.relu(self.a1(input_ids))\n",
        "        adversary_output = self.a2(adversary)\n",
        "\n",
        "        return adversary_output\n"
      ],
      "metadata": {
        "id": "GsvoUU55wMN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conduct_validation(net, data_loader, adv = False):\n",
        "\n",
        "    eval_loss, eval_accuracy, eval_precision, eval_recall, eval_f1 = 0, 0, 0, 0, 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    predictions_net = np.empty((0,))\n",
        "    truths = np.empty((0,))\n",
        "    identities = np.empty((0,))\n",
        "    correct_net = 0\n",
        "    total = 0\n",
        "\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "      for index, data in enumerate(data_loader):\n",
        "\n",
        "        text, fake_truth, female_truth = data\n",
        "\n",
        "        text = text.to(device)\n",
        "        fake_truth = fake_truth.to(device)\n",
        "        female_truth = female_truth.to(device)\n",
        "\n",
        "        if adv:\n",
        "          net_outputs, net_prev_outputs = net(text)\n",
        "        else:\n",
        "          net_outputs = net(text)\n",
        "        _, net_predicted = torch.max(net_outputs.data, 1)\n",
        "\n",
        "        batch_size = fake_truth.size(0)\n",
        "        total += batch_size\n",
        "        correct_net_batch = (net_predicted == fake_truth).sum().item()\n",
        "        correct_net += correct_net_batch\n",
        "\n",
        "\n",
        "        predictions_net = np.concatenate((predictions_net, net_predicted.cpu().numpy()))\n",
        "        truths = np.concatenate((truths, fake_truth.cpu().numpy()))\n",
        "        identities = np.concatenate((identities, female_truth.cpu().numpy()))\n",
        "\n",
        "        pred = net_predicted.detach().cpu().numpy()\n",
        "        label_ids = fake_truth.to('cpu').numpy()\n",
        "\n",
        "        tmp_eval_accuracy, tmp_eval_precision, temp_eval_recall, tmp_eval_f1 = get_metrics(label_ids, pred)\n",
        "\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        eval_precision += tmp_eval_precision\n",
        "        eval_recall += temp_eval_recall\n",
        "        eval_f1 += tmp_eval_f1\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    f1_score = eval_f1/nb_eval_steps\n",
        "    prec_score = eval_precision/nb_eval_steps\n",
        "    recall_score = eval_recall/nb_eval_steps\n",
        "    acc_score = eval_accuracy/nb_eval_steps\n",
        "\n",
        "    print(\"F1 Score: \", f1_score)\n",
        "    print(\"Precision Score: \", prec_score)\n",
        "    print(\"Recall Score: \", recall_score)\n",
        "    print(\"Acc Score: \", acc_score, \"\\n\\n\")\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    return (predictions_net, truths, identities, acc_score)"
      ],
      "metadata": {
        "id": "af0GSLvFwMN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_classifier(clf, optimizer_clf, train_loader, loss_criterion, epochs):\n",
        "\n",
        "  pretrain_classifier_loss = 0\n",
        "  steps = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    print(\"Epoch: \", epoch + 1)\n",
        "    epoch_loss = 0\n",
        "    epoch_batches = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, fake_true, female_true = data\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        fake_true = fake_true.to(device)\n",
        "        female_true = female_true.to(device)\n",
        "\n",
        "        optimizer_clf.zero_grad()\n",
        "\n",
        "        classifier_output, _ = clf(inputs)\n",
        "        classifier_loss = loss_criterion(classifier_output, fake_true)\n",
        "        classifier_loss.backward()\n",
        "        optimizer_clf.step()\n",
        "        pretrain_classifier_loss += classifier_loss.item()\n",
        "        epoch_loss += classifier_loss.item()\n",
        "        epoch_batches += 1\n",
        "        steps += 1\n",
        "\n",
        "    print(\"Average Pretrain Classifier epoch loss: \", epoch_loss/epoch_batches)\n",
        "  print(\"Average Pretrain Classifier batch loss: \", pretrain_classifier_loss/steps)\n",
        "\n",
        "  return clf"
      ],
      "metadata": {
        "id": "LHTIYeTSwMN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, epochs):\n",
        "\n",
        "  pretrain_adversary_loss = 0\n",
        "  steps = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    print(\"Epoch: \", epoch + 1)\n",
        "    epoch_loss = 0\n",
        "    epoch_batches = 0\n",
        "    for i, data in enumerate(train_loader):\n",
        "\n",
        "        inputs, fake_true, female_true = data\n",
        "        inputs = inputs.to(device)\n",
        "        fake_true = fake_true.to(device)\n",
        "        female_true = female_true.to(device)\n",
        "\n",
        "        optimizer_adv.zero_grad()\n",
        "\n",
        "        _, classifier_prev_output = clf(inputs)\n",
        "        adversary_output = adv(classifier_prev_output)\n",
        "        adversary_loss = loss_criterion(adversary_output, female_true)\n",
        "        adversary_loss.backward()\n",
        "        optimizer_adv.step()\n",
        "        pretrain_adversary_loss += adversary_loss.item()\n",
        "        epoch_loss += adversary_loss.item()\n",
        "        epoch_batches += 1\n",
        "        steps += 1\n",
        "\n",
        "    print(\"Average Pretrain Adversary epoch loss: \", epoch_loss/epoch_batches)\n",
        "  print(\"Average Pretrain Adversary batch loss: \", pretrain_adversary_loss/steps)\n",
        "\n",
        "  return adv"
      ],
      "metadata": {
        "id": "3ZeEtp2fwMN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, epochs=1):\n",
        "\n",
        "  adv_loss = 0\n",
        "  steps = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, fake_true, female_true = data\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        fake_true = fake_true.to(device)\n",
        "        female_true = female_true.to(device)\n",
        "\n",
        "        optimizer_adv.zero_grad()\n",
        "\n",
        "        classifier_output, classifier_prev_output = clf(inputs)\n",
        "        adversary_output = adv(classifier_prev_output)\n",
        "        adversary_loss = loss_criterion(adversary_output, female_true)\n",
        "        adversary_loss.backward()\n",
        "        optimizer_adv.step()\n",
        "        adv_loss += adversary_loss.item()\n",
        "        steps += 1\n",
        "\n",
        "  print(\"Average Adversary batch loss: \", adv_loss/steps)\n",
        "\n",
        "  return adv"
      ],
      "metadata": {
        "id": "sTxRDDgiwMN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(clf, optimizer_clf, adv, train_loader, loss_criterion, lbda):\n",
        "\n",
        "  for i, data in enumerate(train_loader):\n",
        "\n",
        "      inputs, fake_true, female_true = data\n",
        "      inputs = inputs.to(device)\n",
        "\n",
        "      fake_true = fake_true.to(device)\n",
        "      female_true = female_true.to(device)\n",
        "\n",
        "\n",
        "\n",
        "      optimizer_clf.zero_grad()\n",
        "\n",
        "      classifier_output, classifier_prev_output = clf(inputs)\n",
        "      adversary_output = adv(classifier_prev_output)\n",
        "      adversary_loss = loss_criterion(adversary_output, female_true)\n",
        "      classifier_loss = loss_criterion(classifier_output, fake_true)\n",
        "      total_classifier_loss = classifier_loss - lbda * adversary_loss\n",
        "      total_classifier_loss.backward()\n",
        "\n",
        "      optimizer_clf.step()\n",
        "\n",
        "      print(\"Adversary Mini-Batch loss: \", adversary_loss.item())\n",
        "      print(\"Classifier Mini-Batch loss: \", classifier_loss.item())\n",
        "      print(\"Total Mini-Batch loss: \", total_classifier_loss.item())\n",
        "\n",
        "      break\n",
        "\n",
        "  return clf"
      ],
      "metadata": {
        "id": "GdPuRMM6wMN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_params = [0.1, 0.2, 0.3, 0.5 1,2,3,5,7,10]\n",
        "\n",
        "lbda_train_accs = []\n",
        "lbda_valid_accs = []\n",
        "protected_fake_rates = []\n",
        "unprotected_fake_rates = []\n",
        "protected_tp_rates = []\n",
        "unprotected_tp_rates = []\n",
        "protected_fp_rates = []\n",
        "unprotected_fp_rates = []\n",
        "demo_parity_scores = []\n",
        "tp_parity_scores = []\n",
        "fp_parity_scores = []\n",
        "equ_odds_scores = []\n",
        "\n",
        "for lbda in lambda_params:\n",
        "  print('Lambda: ' + str(lbda))\n",
        "\n",
        "  clf = Classifier(fakness_labels = 2)\n",
        "  adv = Adversary(identity_labels = 2)\n",
        "\n",
        "  loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "  optimizer_adv = optim.Adam(adv.parameters(), lr=0.001)\n",
        "\n",
        "  lrlast = .001\n",
        "  lrmain = .00001\n",
        "  optimizer_clf = optim.Adam(\n",
        "      [\n",
        "          {\"params\":clf.bert.parameters(),\"lr\": lrmain},\n",
        "          {\"params\":clf.c1.parameters(), \"lr\": lrlast},\n",
        "\n",
        "      {\"params\":clf.c3.parameters(), \"lr\": lrlast}\n",
        "    ])\n",
        "\n",
        "  clf.to(device)\n",
        "  adv.to(device)\n",
        "\n",
        "\n",
        "\n",
        "  for param in adv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  clf = pretrain_classifier(clf, optimizer_clf, train_loader, loss_criterion, 3)\n",
        "\n",
        "  for param in adv.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "  for param in clf.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  adv = pretrain_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, 3)\n",
        "\n",
        "  for param in clf.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "  train_accs = []\n",
        "  valid_accs = []\n",
        "  iterations = 20\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "      print(\"Iteration: \", iteration)\n",
        "\n",
        "\n",
        "\n",
        "      for param in clf.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "      adv = train_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, epochs=1)\n",
        "\n",
        "      for param in clf.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "      for param in adv.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "      clf = train_classifier(clf, optimizer_clf, adv, train_loader, loss_criterion, lbda)\n",
        "\n",
        "      for param in adv.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "      if (iteration + 1) % 2 == 0:\n",
        "\n",
        "        print('Training metrics:')\n",
        "        y_pred, actual_labels, protected_labels, acc_score = conduct_validation(clf, train_loader, adv = True)\n",
        "        train_accs.append(acc_score)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Fairness Metrics on Train:\")\n",
        "        non_protected_labels = np.asarray(get_unprotected_class(protected_labels))\n",
        "        thres = 0.5\n",
        "        female_tox_rate, nf_tox_rate, female_tp_rate, nf_tp_rate, female_fp_rate, nf_fp_rate, demo_parity, tp_parity, fp_parity, equ_odds =\\\n",
        "        get_fairness_metrics(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "\n",
        "        print(\"Fake Prediction Rates: \", \"Female -\", female_tox_rate, \"Non-Female - \", nf_tox_rate)\n",
        "        print(\"True Positive Prediction Rates: \", \"Female -\", female_tp_rate, \"Non-Female - \", nf_tp_rate)\n",
        "        print(\"False Positive Prediction Rates: \", \"Female -\", female_fp_rate, \"Non-Female - \", nf_fp_rate)\n",
        "        print(\"Demographic Parity: \", demo_parity)\n",
        "        print(\"True Positive Parity: \", tp_parity)\n",
        "        print(\"False Positive Parity: \", fp_parity)\n",
        "        print(\"Equalized Odds: \", equ_odds)\n",
        "        print(\"\\n\")\n",
        "        print('Validation metrics:')\n",
        "        y_pred, actual_labels, protected_labels, acc_score = conduct_validation(clf, val_loader, adv = True)\n",
        "        valid_accs.append(acc_score)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Fairness Metrics on Validation:\")\n",
        "        non_protected_labels = np.asarray(get_unprotected_class(protected_labels))\n",
        "        thres = 0.5\n",
        "        female_tox_rate, nf_tox_rate, female_tp_rate, nf_tp_rate, female_fp_rate, nf_fp_rate, demo_parity, tp_parity, fp_parity, equ_odds =\\\n",
        "        get_fairness_metrics(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "\n",
        "        print(\"Fake Prediction Rates: \", \"Female -\", female_tox_rate, \"Non-Female - \", nf_tox_rate)\n",
        "        print(\"True Positive Prediction Rates: \", \"Female -\", female_tp_rate, \"Non-Female - \", nf_tp_rate)\n",
        "        print(\"False Positive Prediction Rates: \", \"Female -\", female_fp_rate, \"Non-Female - \", nf_fp_rate)\n",
        "        print(\"Demographic Parity: \", demo_parity)\n",
        "        print(\"True Positive Parity: \", tp_parity)\n",
        "        print(\"False Positive Parity: \", fp_parity)\n",
        "        print(\"Equalized Odds: \", equ_odds)\n",
        "        print(\"\\n\\n\\n__________________\")\n",
        "\n",
        "        if iteration == iterations -1:\n",
        "          protected_fake_rates.append(female_tox_rate)\n",
        "          unprotected_fake_rates.append(nf_tox_rate)\n",
        "          protected_tp_rates.append(female_tp_rate)\n",
        "          unprotected_tp_rates.append(nf_tp_rate)\n",
        "          protected_fp_rates.append(female_fp_rate)\n",
        "          unprotected_fp_rates.append(nf_fp_rate)\n",
        "          demo_parity_scores.append(demo_parity)\n",
        "          tp_parity_scores.append(tp_parity)\n",
        "          fp_parity_scores.append(fp_parity)\n",
        "          equ_odds_scores.append(equ_odds)\n",
        "\n",
        "  lbda_train_accs.append(train_accs)\n",
        "  lbda_valid_accs.append(valid_accs)\n",
        "\n",
        "\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "g2SoqBVZwMN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(clf.state_dict(), \"/content/drive/MyDrive/Gender Bias/saved_models/SC_Classifier_Final_covid\")\n",
        "torch.save(adv.state_dict(), \"/content/drive/MyDrive/Gender Bias/saved_models/SC_Adversary_Final_covid\")"
      ],
      "metadata": {
        "id": "LidOdSK7wMN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ISOT"
      ],
      "metadata": {
        "id": "eR2YyKU6G-3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv ('/content/drive/MyDrive/Gender Bias/isot.csv')"
      ],
      "metadata": {
        "id": "zKcI34GqG-3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {'fake': 1, 'real': 0}\n",
        "df['label'] = df['label'].map(label_mapping)"
      ],
      "metadata": {
        "id": "lP32JgHRG-3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['female'] = df['gender'].apply(lambda x: 1 if x == 'female' else 0)\n",
        "df['male'] = df['gender'].apply(lambda x: 1 if x == 'male' else 0)"
      ],
      "metadata": {
        "id": "uzHNLiIiG-3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_total , df_val_total = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "GKa6MeuxG-3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_total"
      ],
      "metadata": {
        "id": "FhdXBw3YG-3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train_total\n",
        "df_val = df_val_total"
      ],
      "metadata": {
        "id": "K636RyFkG-3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments_train = df_train.tweet\n",
        "comments_val = df_val.tweet"
      ],
      "metadata": {
        "id": "HkDi0dpHG-3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_female_gender(x):\n",
        "  if np.isnan(x.female) or x.female < 0.5:\n",
        "      return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "fYQVN0X0G-3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unprotected_class(list_of_protected):\n",
        "  new = [1 if i == 0 else 0 for i in list_of_protected]\n",
        "  return new"
      ],
      "metadata": {
        "id": "COm4E-o7G-3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(labels, preds):\n",
        "  pred_flat = preds.flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "\n",
        "  acc = accuracy_score(labels_flat, pred_flat)\n",
        "  pre = precision_score(labels_flat, pred_flat)\n",
        "  rec = recall_score(labels_flat, pred_flat)\n",
        "  f1 = f1_score(labels_flat, pred_flat, average=\"weighted\")\n",
        "\n",
        "  return acc, pre, rec, f1"
      ],
      "metadata": {
        "id": "wwdlyDU1G-3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fakness_labels_train = list(df_train.label.apply(lambda x: 1 if x >= 0.5 else 0))\n",
        "identity_labels_train = list(df_train.apply(extract_female_gender, axis = 1))\n",
        "fakness_labels_val = list(df_val.label.apply(lambda x: 1 if x >= 0.5 else 0))\n",
        "identity_labels_val = list(df_val.apply(extract_female_gender, axis = 1))\n",
        "unprotected_labels_train = get_unprotected_class(identity_labels_train)\n",
        "unprotected_labels_val = get_unprotected_class(identity_labels_val)"
      ],
      "metadata": {
        "id": "6iZ2Jc2OG-3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(comments_train), len(fakness_labels_train))\n",
        "print(comments_train[:10])\n",
        "print(fakness_labels_train[:10])\n",
        "print(identity_labels_train[:10])"
      ],
      "metadata": {
        "id": "hhWGo1fvG-3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 128\n",
        "SEED = 519\n",
        "BATCH_SIZE = 32\n",
        "BERT_MODEL_PATH = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)"
      ],
      "metadata": {
        "id": "_8JCXO_EG-3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_lines(example, max_seq_length,tokenizer):\n",
        "    all_tokens = []\n",
        "    longer = 0\n",
        "    for text in tqdm(example):\n",
        "        tokens_a = tokenizer.tokenize(text)\n",
        "        if len(tokens_a)>max_seq_length:\n",
        "            tokens_a = tokens_a[:max_seq_length]\n",
        "            longer += 1\n",
        "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
        "        all_tokens.append(one_token)\n",
        "    print(\"Tokens longer than max_length: \", longer)\n",
        "    return np.array(all_tokens)\n"
      ],
      "metadata": {
        "id": "URj2R9GnG-3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_train = convert_lines(comments_train.fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
        "fakness_labels_train = torch.tensor(fakness_labels_train)\n",
        "female_labels_train = torch.tensor(identity_labels_train)\n",
        "\n",
        "input_val = convert_lines(comments_val.fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
        "fakness_labels_val = torch.tensor(fakness_labels_val)\n",
        "female_labels_val = torch.tensor(identity_labels_val)\n"
      ],
      "metadata": {
        "id": "jW7y7E_LG-3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.sum(fakness_labels_train).data)\n",
        "print(torch.sum(female_labels_train).data)\n",
        "\n",
        "print(torch.sum(fakness_labels_val).data)\n",
        "print(torch.sum(female_labels_val).data)\n"
      ],
      "metadata": {
        "id": "Dw3BEceKG-3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.utils.data.TensorDataset(torch.tensor(input_train, dtype=torch.long), fakness_labels_train, female_labels_train)\n",
        "train_loader = torch.utils.data.DataLoader(X_train, batch_size=32, shuffle=True)\n",
        "\n",
        "X_val = torch.utils.data.TensorDataset(torch.tensor(input_val, dtype=torch.long), fakness_labels_val, female_labels_val)\n",
        "val_loader = torch.utils.data.DataLoader(X_val, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "dWJ0ImWtG-3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fairness_metrics(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "  def get_fake_rates(y_pred, protected_labels, non_protected_labels, thres):\n",
        "    protected_ops = y_pred[protected_labels == 1]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[non_protected_labels == 1]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return np.round(protected_prob, 2), np.round(non_protected_prob, 2)\n",
        "\n",
        "  def get_true_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "    protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels == 1)]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 1)]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return np.round(protected_prob, 2), np.round(non_protected_prob, 2)\n",
        "\n",
        "\n",
        "  def get_false_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "    protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels ==0)]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 0)]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return np.round(protected_prob, 2), np.round(non_protected_prob, 2)\n",
        "\n",
        "  def demographic_parity(y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "    protected_ops = y_pred[protected_labels == 1]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[non_protected_labels == 1]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return abs(protected_prob - non_protected_prob)\n",
        "\n",
        "  def true_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "    protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels == 1)]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 1)]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return abs(protected_prob - non_protected_prob)\n",
        "\n",
        "  def false_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "\n",
        "    protected_ops = y_pred[np.bitwise_and(protected_labels == 1, actual_labels ==0)]\n",
        "    protected_prob = sum(protected_ops)/len(protected_ops)\n",
        "\n",
        "    non_protected_ops = y_pred[np.bitwise_and(non_protected_labels == 1, actual_labels == 0)]\n",
        "    non_protected_prob = sum(non_protected_ops)/len(non_protected_ops)\n",
        "\n",
        "    return abs(protected_prob - non_protected_prob)\n",
        "\n",
        "\n",
        "  def equalized_odds(actual_labels, y_pred, protected_labels, non_protected_labels, thres):\n",
        "    return true_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres) + false_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "\n",
        "  female_tox_rate, nf_tox_rate = get_fake_rates(y_pred, protected_labels, non_protected_labels, thres)\n",
        "  female_tp_rate, nf_tp_rate = get_true_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "  female_fp_rate, nf_fp_rate = get_false_positive_rates(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "  demo_parity = demographic_parity(y_pred, protected_labels, non_protected_labels, thres)\n",
        "  tp_parity = true_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "  fp_parity = false_positive_parity(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "  equ_odds = equalized_odds(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "\n",
        "  return female_tox_rate, nf_tox_rate, female_tp_rate, nf_tp_rate, female_fp_rate, nf_fp_rate, demo_parity, tp_parity, fp_parity, equ_odds\n",
        "\n"
      ],
      "metadata": {
        "id": "T70A_EynG-3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072,\n",
        "        hidden_dropout_prob=0.1)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, fakness_labels = 2):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.c1 = nn.Linear(config.hidden_size, 324)\n",
        "        self.c3 = nn.Linear(324, fakness_labels)\n",
        "\n",
        "        nn.init.xavier_normal_(self.c1.weight)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "\n",
        "        classifier_prev_output = F.relu(self.c1(pooled_output))\n",
        "\n",
        "        classifier_output = self.c3(classifier_prev_output)\n",
        "\n",
        "        return classifier_output, classifier_prev_output\n",
        "\n",
        "class Adversary(nn.Module):\n",
        "    def __init__(self, identity_labels = 2):\n",
        "        super(Adversary, self).__init__()\n",
        "\n",
        "        self.a1 = nn.Linear(324,120)\n",
        "        self.a2 = nn.Linear(120, identity_labels)\n",
        "\n",
        "        nn.init.xavier_normal_(self.a1.weight)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        adversary = F.relu(self.a1(input_ids))\n",
        "        adversary_output = self.a2(adversary)\n",
        "\n",
        "        return adversary_output\n"
      ],
      "metadata": {
        "id": "exkNZGinG-3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conduct_validation(net, data_loader, adv = False):\n",
        "\n",
        "    eval_loss, eval_accuracy, eval_precision, eval_recall, eval_f1 = 0, 0, 0, 0, 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    predictions_net = np.empty((0,))\n",
        "    truths = np.empty((0,))\n",
        "    identities = np.empty((0,))\n",
        "    correct_net = 0\n",
        "    total = 0\n",
        "\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "      for index, data in enumerate(data_loader):\n",
        "\n",
        "        text, fake_truth, female_truth = data\n",
        "\n",
        "        text = text.to(device)\n",
        "        fake_truth = fake_truth.to(device)\n",
        "        female_truth = female_truth.to(device)\n",
        "\n",
        "        if adv:\n",
        "          net_outputs, net_prev_outputs = net(text)\n",
        "        else:\n",
        "          net_outputs = net(text)\n",
        "        _, net_predicted = torch.max(net_outputs.data, 1)\n",
        "\n",
        "        batch_size = fake_truth.size(0)\n",
        "        total += batch_size\n",
        "        correct_net_batch = (net_predicted == fake_truth).sum().item()\n",
        "        correct_net += correct_net_batch\n",
        "\n",
        "\n",
        "        predictions_net = np.concatenate((predictions_net, net_predicted.cpu().numpy()))\n",
        "        truths = np.concatenate((truths, fake_truth.cpu().numpy()))\n",
        "        identities = np.concatenate((identities, female_truth.cpu().numpy()))\n",
        "\n",
        "        pred = net_predicted.detach().cpu().numpy()\n",
        "        label_ids = fake_truth.to('cpu').numpy()\n",
        "\n",
        "        tmp_eval_accuracy, tmp_eval_precision, temp_eval_recall, tmp_eval_f1 = get_metrics(label_ids, pred)\n",
        "\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        eval_precision += tmp_eval_precision\n",
        "        eval_recall += temp_eval_recall\n",
        "        eval_f1 += tmp_eval_f1\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    f1_score = eval_f1/nb_eval_steps\n",
        "    prec_score = eval_precision/nb_eval_steps\n",
        "    recall_score = eval_recall/nb_eval_steps\n",
        "    acc_score = eval_accuracy/nb_eval_steps\n",
        "\n",
        "    print(\"F1 Score: \", f1_score)\n",
        "    print(\"Precision Score: \", prec_score)\n",
        "    print(\"Recall Score: \", recall_score)\n",
        "    print(\"Acc Score: \", acc_score, \"\\n\\n\")\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    return (predictions_net, truths, identities, acc_score)"
      ],
      "metadata": {
        "id": "GZm2u2lKG-3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_classifier(clf, optimizer_clf, train_loader, loss_criterion, epochs):\n",
        "\n",
        "  pretrain_classifier_loss = 0\n",
        "  steps = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    print(\"Epoch: \", epoch + 1)\n",
        "    epoch_loss = 0\n",
        "    epoch_batches = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "\n",
        "        inputs, fake_true, female_true = data\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        fake_true = fake_true.to(device)\n",
        "        female_true = female_true.to(device)\n",
        "\n",
        "        optimizer_clf.zero_grad()\n",
        "\n",
        "        classifier_output, _ = clf(inputs)\n",
        "        classifier_loss = loss_criterion(classifier_output, fake_true)\n",
        "        classifier_loss.backward()\n",
        "        optimizer_clf.step()\n",
        "        pretrain_classifier_loss += classifier_loss.item()\n",
        "        epoch_loss += classifier_loss.item()\n",
        "        epoch_batches += 1\n",
        "        steps += 1\n",
        "\n",
        "    print(\"Average Pretrain Classifier epoch loss: \", epoch_loss/epoch_batches)\n",
        "  print(\"Average Pretrain Classifier batch loss: \", pretrain_classifier_loss/steps)\n",
        "\n",
        "  return clf"
      ],
      "metadata": {
        "id": "KFqvZCMGG-3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, epochs):\n",
        "\n",
        "  pretrain_adversary_loss = 0\n",
        "  steps = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    print(\"Epoch: \", epoch + 1)\n",
        "    epoch_loss = 0\n",
        "    epoch_batches = 0\n",
        "    for i, data in enumerate(train_loader):\n",
        "\n",
        "        inputs, fake_true, female_true = data\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        fake_true = fake_true.to(device)\n",
        "        female_true = female_true.to(device)\n",
        "\n",
        "        optimizer_adv.zero_grad()\n",
        "\n",
        "        _, classifier_prev_output = clf(inputs)\n",
        "        adversary_output = adv(classifier_prev_output)\n",
        "        adversary_loss = loss_criterion(adversary_output, female_true)\n",
        "        adversary_loss.backward()\n",
        "        optimizer_adv.step()\n",
        "        pretrain_adversary_loss += adversary_loss.item()\n",
        "        epoch_loss += adversary_loss.item()\n",
        "        epoch_batches += 1\n",
        "        steps += 1\n",
        "\n",
        "    print(\"Average Pretrain Adversary epoch loss: \", epoch_loss/epoch_batches)\n",
        "  print(\"Average Pretrain Adversary batch loss: \", pretrain_adversary_loss/steps)\n",
        "\n",
        "  return adv"
      ],
      "metadata": {
        "id": "fTCKWAczG-3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, epochs=1):\n",
        "\n",
        "  adv_loss = 0\n",
        "  steps = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for i, data in enumerate(train_loader):\n",
        "\n",
        "        inputs, fake_true, female_true = data\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        fake_true = fake_true.to(device)\n",
        "        female_true = female_true.to(device)\n",
        "\n",
        "        optimizer_adv.zero_grad()\n",
        "\n",
        "        classifier_output, classifier_prev_output = clf(inputs)\n",
        "        adversary_output = adv(classifier_prev_output)\n",
        "        adversary_loss = loss_criterion(adversary_output, female_true)\n",
        "        adversary_loss.backward()\n",
        "        optimizer_adv.step()\n",
        "        adv_loss += adversary_loss.item()\n",
        "        steps += 1\n",
        "\n",
        "  print(\"Average Adversary batch loss: \", adv_loss/steps)\n",
        "\n",
        "  return adv"
      ],
      "metadata": {
        "id": "F71afIHzG-3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(clf, optimizer_clf, adv, train_loader, loss_criterion, lbda):\n",
        "\n",
        "  for i, data in enumerate(train_loader):\n",
        "\n",
        "      inputs, fake_true, female_true = data\n",
        "      inputs = inputs.to(device)\n",
        "\n",
        "      fake_true = fake_true.to(device)\n",
        "      female_true = female_true.to(device)\n",
        "\n",
        "\n",
        "\n",
        "      optimizer_clf.zero_grad()\n",
        "\n",
        "      classifier_output, classifier_prev_output = clf(inputs)\n",
        "      adversary_output = adv(classifier_prev_output)\n",
        "      adversary_loss = loss_criterion(adversary_output, female_true)\n",
        "      classifier_loss = loss_criterion(classifier_output, fake_true)\n",
        "      total_classifier_loss = classifier_loss - lbda * adversary_loss\n",
        "      total_classifier_loss.backward()\n",
        "\n",
        "      optimizer_clf.step()\n",
        "\n",
        "      print(\"Adversary Mini-Batch loss: \", adversary_loss.item())\n",
        "      print(\"Classifier Mini-Batch loss: \", classifier_loss.item())\n",
        "      print(\"Total Mini-Batch loss: \", total_classifier_loss.item())\n",
        "\n",
        "      break\n",
        "\n",
        "  return clf"
      ],
      "metadata": {
        "id": "0UTy278OG-3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_params = [0.1, 0.2, 0.3, 0.5 1,2,3,5,7,10]\n",
        "\n",
        "lbda_train_accs = []\n",
        "lbda_valid_accs = []\n",
        "protected_fake_rates = []\n",
        "unprotected_fake_rates = []\n",
        "protected_tp_rates = []\n",
        "unprotected_tp_rates = []\n",
        "protected_fp_rates = []\n",
        "unprotected_fp_rates = []\n",
        "demo_parity_scores = []\n",
        "tp_parity_scores = []\n",
        "fp_parity_scores = []\n",
        "equ_odds_scores = []\n",
        "\n",
        "for lbda in lambda_params:\n",
        "  print('Lambda: ' + str(lbda))\n",
        "\n",
        "\n",
        "  clf = Classifier(fakness_labels = 2)\n",
        "  adv = Adversary(identity_labels = 2)\n",
        "\n",
        "  loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "  optimizer_adv = optim.Adam(adv.parameters(), lr=0.001)\n",
        "\n",
        "  lrlast = .001\n",
        "  lrmain = .00001\n",
        "  optimizer_clf = optim.Adam(\n",
        "      [\n",
        "          {\"params\":clf.bert.parameters(),\"lr\": lrmain},\n",
        "          {\"params\":clf.c1.parameters(), \"lr\": lrlast},\n",
        "\n",
        "      {\"params\":clf.c3.parameters(), \"lr\": lrlast}\n",
        "    ])\n",
        "\n",
        "  clf.to(device)\n",
        "  adv.to(device)\n",
        "\n",
        "\n",
        "\n",
        "  for param in adv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  clf = pretrain_classifier(clf, optimizer_clf, train_loader, loss_criterion, 3)\n",
        "\n",
        "  for param in adv.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "  for param in clf.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  adv = pretrain_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, 3)\n",
        "\n",
        "  for param in clf.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "  train_accs = []\n",
        "  valid_accs = []\n",
        "  iterations = 20\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "      print(\"Iteration: \", iteration)\n",
        "\n",
        "\n",
        "\n",
        "      for param in clf.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "      adv = train_adversary(adv, clf, optimizer_adv, train_loader, loss_criterion, epochs=1)\n",
        "\n",
        "      for param in clf.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "      for param in adv.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "      clf = train_classifier(clf, optimizer_clf, adv, train_loader, loss_criterion, lbda)\n",
        "\n",
        "      for param in adv.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "      if (iteration + 1) % 2 == 0:\n",
        "\n",
        "        print('Training metrics:')\n",
        "        y_pred, actual_labels, protected_labels, acc_score = conduct_validation(clf, train_loader, adv = True)\n",
        "        train_accs.append(acc_score)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Fairness Metrics on Train:\")\n",
        "        non_protected_labels = np.asarray(get_unprotected_class(protected_labels))\n",
        "        thres = 0.5\n",
        "        female_tox_rate, nf_tox_rate, female_tp_rate, nf_tp_rate, female_fp_rate, nf_fp_rate, demo_parity, tp_parity, fp_parity, equ_odds =\\\n",
        "        get_fairness_metrics(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "\n",
        "        print(\"Fake Prediction Rates: \", \"Female -\", female_tox_rate, \"Non-Female - \", nf_tox_rate)\n",
        "        print(\"True Positive Prediction Rates: \", \"Female -\", female_tp_rate, \"Non-Female - \", nf_tp_rate)\n",
        "        print(\"False Positive Prediction Rates: \", \"Female -\", female_fp_rate, \"Non-Female - \", nf_fp_rate)\n",
        "        print(\"Demographic Parity: \", demo_parity)\n",
        "        print(\"True Positive Parity: \", tp_parity)\n",
        "        print(\"False Positive Parity: \", fp_parity)\n",
        "        print(\"Equalized Odds: \", equ_odds)\n",
        "        print(\"\\n\")\n",
        "        print('Validation metrics:')\n",
        "        y_pred, actual_labels, protected_labels, acc_score = conduct_validation(clf, val_loader, adv = True)\n",
        "        valid_accs.append(acc_score)\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Fairness Metrics on Validation:\")\n",
        "        non_protected_labels = np.asarray(get_unprotected_class(protected_labels))\n",
        "        thres = 0.5\n",
        "        female_tox_rate, nf_tox_rate, female_tp_rate, nf_tp_rate, female_fp_rate, nf_fp_rate, demo_parity, tp_parity, fp_parity, equ_odds =\\\n",
        "        get_fairness_metrics(actual_labels, y_pred, protected_labels, non_protected_labels, thres)\n",
        "\n",
        "        print(\"Fake Prediction Rates: \", \"Female -\", female_tox_rate, \"Non-Female - \", nf_tox_rate)\n",
        "        print(\"True Positive Prediction Rates: \", \"Female -\", female_tp_rate, \"Non-Female - \", nf_tp_rate)\n",
        "        print(\"False Positive Prediction Rates: \", \"Female -\", female_fp_rate, \"Non-Female - \", nf_fp_rate)\n",
        "        print(\"Demographic Parity: \", demo_parity)\n",
        "        print(\"True Positive Parity: \", tp_parity)\n",
        "        print(\"False Positive Parity: \", fp_parity)\n",
        "        print(\"Equalized Odds: \", equ_odds)\n",
        "        print(\"\\n\\n\\n__________________\")\n",
        "\n",
        "        if iteration == iterations -1:\n",
        "          protected_fake_rates.append(female_tox_rate)\n",
        "          unprotected_fake_rates.append(nf_tox_rate)\n",
        "          protected_tp_rates.append(female_tp_rate)\n",
        "          unprotected_tp_rates.append(nf_tp_rate)\n",
        "          protected_fp_rates.append(female_fp_rate)\n",
        "          unprotected_fp_rates.append(nf_fp_rate)\n",
        "          demo_parity_scores.append(demo_parity)\n",
        "          tp_parity_scores.append(tp_parity)\n",
        "          fp_parity_scores.append(fp_parity)\n",
        "          equ_odds_scores.append(equ_odds)\n",
        "\n",
        "  lbda_train_accs.append(train_accs)\n",
        "  lbda_valid_accs.append(valid_accs)\n",
        "\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "uXFA13vrG-3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(clf.state_dict(), \"/content/drive/MyDrive/Gender Bias/saved_models/SC_Classifier_Final_ISOT\")\n",
        "torch.save(adv.state_dict(), \"/content/drive/MyDrive/Gender Bias/saved_models/SC_Adversary_Final_ISOT\")"
      ],
      "metadata": {
        "id": "PXi_ygmCG-3e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}